# -*- coding: utf-8 -*-
"""Copy of Queries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16dOcEvYc1TWinRIy2kejBGr9U9spC0ra
"""

pip install rank_bm25

import json
import argparse
import random
import string
import nltk
from nltk.corpus import stopwords
from rank_bm25 import BM25Okapi
from tqdm import tqdm
import spacy

# Download data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
nlp = spacy.load("en_core_web_sm")
with open("pairs_new.json") as f:
    pairs = json.load(f)

# Configuration
REMOVE_PUNCTUATION = True
REMOVE_STOPWORDS = True
LOWERCASE_QUERY = True
REMOVE_FUNCTION_WORDS = True
RESULTS_LIMIT = 50

# Basic preprocessing function
def preprocess(text):
    if REMOVE_PUNCTUATION:
        text = ''.join(' ' if c in string.punctuation else c for c in text)
    if LOWERCASE_QUERY:
        text = text.lower()
    if REMOVE_STOPWORDS:
        stop_words = set(stopwords.words("english"))
        text = " ".join(word for word in text.split() if word.lower() not in stop_words)
    if REMOVE_FUNCTION_WORDS:
        spacy_doc = nlp(text)
        text = " ".join(token.text for token in spacy_doc if token.pos_ in ["NOUN", "ADJ", "VERB"])
    return text

# ID mappings
id_to_details = {entry['id']: entry['details'] for entry in pairs}
id_to_query = {entry['id']: entry['query'] for entry in pairs}
all_ids = list(id_to_query.keys())

# Perform preprocessing
print("Preprocessing documents...")
tokenized_corpus = []
id_order = []
for entry in tqdm(pairs):
    docid = entry['id']
    processed = preprocess(entry['details'])
    tokens = nltk.word_tokenize(processed)
    tokenized_corpus.append(tokens)
    id_order.append(docid)

# Perform BM25 search
print("Building BM25 index...")
bm25 = BM25Okapi(tokenized_corpus)

# Generate hard negatives
print("Scoring queries...")
for entry in tqdm(pairs):
    qid = entry['id']
    processed_query = preprocess(entry['query'])
    tokenized_query = nltk.word_tokenize(processed_query)
    scores = bm25.get_scores(tokenized_query)

    scored = list(zip(id_order, scores))
    scored_sorted = sorted(scored, key=lambda x: x[1], reverse=True)

    top_doc, top_score = scored_sorted[0]

    if top_doc != qid:
        self_score = dict(scored).get(qid, -1)
        negatives = [doc for doc, score in scored_sorted if doc != qid and score >= self_score][:10]
    else:
        # Add fallback in case BM25 retrieves the document correctly
        negatives = random.sample([doc for doc in id_order if doc != qid], min(5, len(id_order) - 1))

    entry["hard_negatives"] = negatives

# Create new file with hard negatives
with open("pairs_with_negatives.json", "w") as f:
    json.dump(pairs, f, indent=2)

print("âœ… Pairs with negatives complete.")
