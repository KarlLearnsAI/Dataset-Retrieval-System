# -*- coding: utf-8 -*-
"""Another copy of Queries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q9Ony9dh6q6UoWd8WGeDyjTXL0o7NOd7
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch, json
from tqdm import tqdm

# Load dataset
with open("dataset-metadata.json") as f:
    data = json.load(f)

# Load model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "tiiuae/falcon-7b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.padding_side = 'left'

model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
model.resize_token_embeddings(len(tokenizer))
model.config.pad_token_id = tokenizer.pad_token_id
model.to(device)

# Batch generation and parsing logic
BATCH_SIZE = 18

# Prompt builder for each dataset
def construct_prompt(details):
    prompt = """1) Extract keyphrases regarding the task (e.g. image classification), data modality (e.g. images or speech), domain (e.g. biomedical or aerial), training style (unsupervised, semi-supervised, fully supervised, or reinforcement learning), text length (sentence-level or paragraph-level), language required (e.g. English)
2) Write a brief, single-sentence summary containing these relevant keyphrases. This summary must describe the task studied in the paper.

Dataset description:
We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).

Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
Task: question generation
Modality: text
Domain: natural language processing
Training Style: fully supervised
Text Length: paragraph-level
Language Required: any language
Single-Sentence Summary: Improved end-to-end system for automatic question generation.
--

Dataset description:
In this paper, we study the actor-action semantic segmentation problem, which requires joint labeling of both actor and action categories in video frames. One major challenge for this task is that when an actor performs an action, different body parts of the actor provide different types of cues for the action category and may receive inconsistent action labeling when they are labeled independently. To address this issue, we propose an end-to-end region-based actor-action segmentation approach which relies on region masks from an instance segmentation algorithm. Our main novelty is to avoid labeling pixels in a region mask independently - instead we assign a single action label to these pixels to achieve consistent action labeling. When a pixel belongs to multiple region masks, max pooling is applied to resolve labeling conflicts. Our approach uses a two-stream network as the front-end (which learns features capturing both appearance and motion information), and uses two region-based segmentation networks as the back-end (which takes the fused features from the two-stream network as the input and predicts actor-action labeling). Experiments on the A2D dataset demonstrate that both the region-based segmentation strategy and the fused features from the two-stream network contribute to the performance improvements. The proposed approach outperforms the state-of-the-art results by more than 8% in mean class accuracy, and more than 5% in mean class IOU, which validates its effectiveness.

Output: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)
Task: actor-action semantic segmentation
Modality: video
Domain: computer vision
Training Style: fully supervised
Text Length: any
Language Required: any
Single-Sentence Summary: Supervised model for actor-action semantic segmentation from video.
--"""

    prompt_tokens = tokenizer(prompt, return_tensors="pt").input_ids.shape[1]
    max_detail_tokens = 1500 - prompt_tokens - 200  # buffer for instruction + output
    detail_tokens = tokenizer(details, return_tensors="pt", truncation=True, max_length=max_detail_tokens, return_overflowing_tokens=False)
    trimmed_details = tokenizer.decode(detail_tokens.input_ids[0], skip_special_tokens=True)

    return prompt + f"\n\nDataset description:\n{trimmed_details}\n\nOutput: (Task | Modality | Domain | Training Style | Text Length |  Language Required | Single-Sentence Summary)\nTask:"



# Extract final query from output
def extract_query(text):
    for line in reversed(text.split("\n")):
        if line.strip().lower().startswith("single-sentence summary:"):
            return line.split(":", 1)[1].strip()
    return None

# Run batch inference and save
with open("queries.jsonl", "w") as outfile:
    for i in tqdm(range(0, len(data), BATCH_SIZE), desc="Generating"):
        batch = data[i:i + BATCH_SIZE]
        prompts = [construct_prompt(x['details']) for x in batch]
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=1500).to(device)
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=128,
            do_sample=False,
            top_p=0.8,
            temperature=0.7)
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        queries = [extract_query(d) for d in decoded]
        for query, entry in zip(queries, batch):
            json.dump({"description": entry['details'], "query": query}, outfile)
            outfile.write("\n")

print("✅ Generation complete.")

def construct_feature_prompt(features):
    prompt = """Given a list of feature names from a dataset, generate a one-sentence summary describing what kind of data the features represent.

Feature names:
"""

    prompt_tokens = tokenizer(prompt, return_tensors="pt").input_ids.shape[1]
    max_feature_tokens = 1500 - prompt_tokens - 200  # buffer for instruction + output
    feature_tokens = tokenizer(", ".join(features), return_tensors="pt", truncation=True, max_length=max_feature_tokens, return_overflowing_tokens=False)
    trimmed_features = tokenizer.decode(feature_tokens.input_ids[0], skip_special_tokens=True)
    return prompt + trimmed_features + "\n\nSummary:"

# Extract final summary from output
def extract_summary(text):
    for line in reversed(text.split("\n")):
        if line.strip().lower().startswith("summary:"):
            return line.split(":", 1)[1].strip()
    return None

with open("feature_summaries.jsonl", "w") as outfile:
    for i in tqdm(range(0, len(data), BATCH_SIZE), desc="Generating feature summaries"):
        batch = data[i:i + BATCH_SIZE]
        prompts = [construct_feature_prompt(x['feature_names']) for x in batch]
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=1500).to(device)
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=64,
            do_sample=False,
            top_p=0.9,
            temperature=0.7
        )
        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        summaries = [extract_summary(d) for d in decoded]
        for summary, entry in zip(summaries, batch):
            json.dump({"features": entry['feature_names'], "summary": summary}, outfile)
            outfile.write("\n")

print("✅ Feature summaries complete.")

# Create new file with feature name summaries inserted into original dataset
with open("feature_summaries.jsonl") as f:
    feature_summaries = [json.loads(line) for line in f]

for doc, fsum in zip(data, feature_summaries):
    doc["feature_summary"] = fsum.get("summary", None)
    doc.pop("feature_names", None)

with open("documents.json", "w") as f:
    json.dump(data, f, indent=2)

print("✅ documents.json file with summarised features created.")

# Create new file combining queries and documents
with open("queries.jsonl") as f:
    queries = [json.loads(line) for line in f]

with open("documents.json") as f:
    documents = json.load(f)

for original, gen in zip(documents, queries):
    original["query"] = gen.get("query", None)

with open("pairs.json", "w") as f:
    json.dump(documents, f, indent=2)

print("✅ pairs.json file with queries successfully created.")

with open("pairs.json") as f:
    pairs = json.load(f)

# Simplify the prompt for items that the model could not generate a proper query for

# Prompt builder for each dataset
def construct_prompt(details):
    prompt = """Write a brief, single-sentence summary containing these relevant keyphrases. This summary must describe the task studied in the paper."""
    prompt_tokens = tokenizer(prompt, return_tensors="pt").input_ids.shape[1]
    max_detail_tokens = 2000 - prompt_tokens - 500  # buffer for instruction + output
    detail_tokens = tokenizer(details, return_tensors="pt", truncation=True, max_length=max_detail_tokens, return_overflowing_tokens=False)
    trimmed_details = tokenizer.decode(detail_tokens.input_ids[0], skip_special_tokens=True)

    return prompt + f"\n\nDataset description:\n{trimmed_details}\n\nSingle-Sentence Summary:"

# Extract final query from output
def extract_query(text):
    return text.split("Single-Sentence Summary:")[-1]

# Select entries with inadequately generated queries
target_queries = ["", "Supervised model for actor-action semantic segmentation from video."]
to_update = [entry for entry in pairs if entry.get("query") in target_queries]

# Generate updated queries
new_queries = []
for entry in tqdm(to_update, desc="Regenerating updated queries"):
    prompt = construct_prompt(entry['details'])
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=2000).to(device)
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=128,
        do_sample=False,
        top_p=0.8,
        temperature=0.7
    )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    updated_query = extract_query(decoded)
    new_queries.append({"id": entry["id"], "query": updated_query})

# Update pairs with new queries and tag inadequate feature summaries with a special token
id_to_new_query = {item["id"]: item["query"] for item in new_queries}

for entry in pairs:
    # Replace query if in new_queries
    if entry["id"] in id_to_new_query:
        entry["query"] = id_to_new_query[entry["id"]]

    # Replace empty feature_summary with [UNAVAILABLE]
    if entry.get("feature_summary", "").strip() == "":
        entry["feature_summary"] = "[UNAVAILABLE]"

# Save updated file
with open("pairs.json", "w") as f:
    json.dump(pairs, f, indent=2)